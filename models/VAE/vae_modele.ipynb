{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import save_image\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. PERCEPTUAL LOSS (VGG) ---\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # On utilise les features de VGG16 (jusqu'√† la couche relu3_3 environ)\n",
    "        vgg = models.vgg16(weights=\"IMAGENET1K_V1\").features[:16]\n",
    "        self.vgg = vgg.eval()\n",
    "        for p in self.vgg.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        return nn.functional.l1_loss(self.vgg(x), self.vgg(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- VAE-UNET HYBRIDE ---\n",
    "class VAE_UNet(nn.Module):\n",
    "    def __init__(self, latent_dim=64):\n",
    "        super(VAE_UNet, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # ENCODEUR avec skip connections\n",
    "        self.enc1 = self.conv_block(3, 64)\n",
    "        self.pool1 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # BOTTLENECK - Espace latent VAE\n",
    "        self.bottleneck = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Couches VAE (mu et logvar)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(1024 * 8 * 8, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(1024 * 8 * 8, latent_dim)\n",
    "        \n",
    "        # Reconstruction depuis l'espace latent\n",
    "        self.fc_decode = nn.Linear(latent_dim, 1024 * 8 * 8)\n",
    "        self.unflatten = nn.Unflatten(1, (1024, 8, 8))\n",
    "        \n",
    "        # D√âCODEUR avec skip connections (style U-Net)\n",
    "        self.up4 = nn.ConvTranspose2d(1024, 512, 2, stride=2)\n",
    "        self.dec4 = self.conv_block(1024, 512)  # 1024 = 512 (up) + 512 (skip)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(512, 256, 2, stride=2)\n",
    "        self.dec3 = self.conv_block(512, 256)  # 512 = 256 (up) + 256 (skip)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(256, 128, 2, stride=2)\n",
    "        self.dec2 = self.conv_block(256, 128)  # 256 = 128 (up) + 128 (skip)\n",
    "        \n",
    "        self.up1 = nn.ConvTranspose2d(128, 64, 2, stride=2)\n",
    "        self.dec1 = self.conv_block(128, 64)  # 128 = 64 (up) + 64 (skip)\n",
    "        \n",
    "        # Sortie finale\n",
    "        self.out = nn.Conv2d(64, 3, 1)\n",
    "    \n",
    "    def conv_block(self, in_ch, out_ch):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(0.2, inplace=True), # LeakyReLU est souvent mieux pour Tanh\n",
    "                nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
    "                nn.BatchNorm2d(out_ch),\n",
    "                nn.LeakyReLU(0.2, inplace=True)\n",
    "            )\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def encode(self, x):\n",
    "        # Encodeur avec sauvegarde des skip connections\n",
    "        e1 = self.enc1(x)          # 128x128x64\n",
    "        p1 = self.pool1(e1)\n",
    "        \n",
    "        e2 = self.enc2(p1)         # 64x64x128\n",
    "        p2 = self.pool2(e2)\n",
    "        \n",
    "        e3 = self.enc3(p2)         # 32x32x256\n",
    "        p3 = self.pool3(e3)\n",
    "        \n",
    "        e4 = self.enc4(p3)         # 16x16x512\n",
    "        p4 = self.pool4(e4)\n",
    "        \n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(p4)    # 8x8x1024\n",
    "        \n",
    "        # VAE latent space\n",
    "        flat = self.flatten(b)\n",
    "        mu = self.fc_mu(flat)\n",
    "        logvar = self.fc_logvar(flat)\n",
    "        \n",
    "        return mu, logvar, e1, e2, e3, e4\n",
    "    \n",
    "    def decode(self, z, e1, e2, e3, e4):\n",
    "        # On repart du vecteur latent\n",
    "        x = self.fc_decode(z)\n",
    "        x = self.unflatten(x) \n",
    "        \n",
    "        # Passage dans le d√©codeur avec Skip Connections\n",
    "        d4 = self.up4(x)\n",
    "        d4 = torch.cat([d4, e4], dim=1)\n",
    "        d4 = self.dec4(d4)\n",
    "        \n",
    "        d3 = self.up3(d4)\n",
    "        d3 = torch.cat([d3, e3], dim=1)\n",
    "        d3 = self.dec3(d3)\n",
    "        \n",
    "        d2 = self.up2(d3)\n",
    "        d2 = torch.cat([d2, e2], dim=1)\n",
    "        d2 = self.dec2(d2)\n",
    "        \n",
    "        d1 = self.up1(d2)\n",
    "        d1 = torch.cat([d1, e1], dim=1)\n",
    "        d1 = self.dec1(d1)\n",
    "        \n",
    "        out = self.out(d1)\n",
    "        return torch.tanh(out) # Tanh selon conseil n¬∞3 (plage -1 √† 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1. On encode\n",
    "        mu, logvar, e1, e2, e3, e4 = self.encode(x)\n",
    "        # 2. On reparam√®tre (le \"bruit\" du VAE)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        # 3. On d√©code avec les skips\n",
    "        recon = self.decode(z, e1, e2, e3, e4)\n",
    "        \n",
    "        return recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. LOSS AVEC ANNEALING ET VGG ---\n",
    "def hybrid_loss(recon, target, mu, logvar, epoch, perceptual_model, beta_max=1e-4):\n",
    "    # KL Annealing : Beta augmente progressivement les 20 premi√®res √©poques\n",
    "    beta = min(beta_max, (epoch + 1) / 20 * beta_max)\n",
    "    \n",
    "    l1 = nn.functional.l1_loss(recon, target)\n",
    "    mse = nn.functional.mse_loss(recon, target)\n",
    "    vgg_loss = perceptual_model(recon, target)\n",
    "    kld = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # √âquilibrage des poids de la loss\n",
    "    total_loss = l1 + 0.5 * mse + 0.1 * vgg_loss + beta * kld\n",
    "    return total_loss, l1.item(), vgg_loss.item(), kld.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RestorationDataset(Dataset):\n",
    "    def __init__(self, degraded_dir, clean_dir, transform=None):\n",
    "        self.degraded_dir = degraded_dir\n",
    "        self.clean_dir = clean_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        print(\"üîç Indexation des fichiers en cours...\")\n",
    "        # On liste les deux dossiers\n",
    "        degraded_list = os.listdir(degraded_dir)\n",
    "        clean_list_set = set(os.listdir(clean_dir)) # TR√àS IMPORTANT : Le set rend la recherche instantan√©e\n",
    "        \n",
    "        self.filenames = []\n",
    "        for f in degraded_list:\n",
    "            # On g√©n√®re le nom correspondant dans le dossier clean\n",
    "            clean_target = f.replace(\"degraded_\", \"\")\n",
    "            if clean_target in clean_list_set:\n",
    "                self.filenames.append(f)\n",
    "        \n",
    "        print(f\"Indexation termin√©e : {len(self.filenames)} paires d'images trouv√©es.\")\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        deg_fn = self.filenames[idx]\n",
    "        cln_fn = deg_fn.replace(\"degraded_\", \"\")\n",
    "        \n",
    "        deg_img = Image.open(os.path.join(self.degraded_dir, deg_fn)).convert('RGB')\n",
    "        cln_img = Image.open(os.path.join(self.clean_dir, cln_fn)).convert('RGB')\n",
    "        \n",
    "        seed = torch.seed()\n",
    "        torch.manual_seed(seed)\n",
    "        deg_img = self.transform(deg_img)\n",
    "        torch.manual_seed(seed)\n",
    "        cln_img = self.transform(cln_img)\n",
    "        \n",
    "        return deg_img, cln_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss_function(recon, target, mu, logvar, beta=0.0001):\n",
    "    \"\"\"\n",
    "    Loss VAE optimis√©e :\n",
    "    - L1 : nettet√©\n",
    "    - MSE : structure\n",
    "    - KL divergence : r√©gularisation\n",
    "    \"\"\"\n",
    "    l1_loss = nn.functional.l1_loss(recon, target, reduction='sum')\n",
    "    mse_loss = nn.functional.mse_loss(recon, target, reduction='sum')\n",
    "    kld_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    total_loss = l1_loss + 0.5 * mse_loss + beta * kld_loss\n",
    "    \n",
    "    return total_loss, l1_loss.item(), mse_loss.item(), kld_loss.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(model, dataloader, epoch, device, output_dir=\"samples\", num_samples=8):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        degraded, clean = next(iter(dataloader))\n",
    "        degraded, clean = degraded[:num_samples].to(device), clean[:num_samples].to(device)\n",
    "        restored, _, _ = model(degraded)\n",
    "        \n",
    "        # --- √âTAPE DE D√â-NORMALISATION ---\n",
    "        # On repasse de [-1, 1] √† [0, 1] pour que l'image soit lisible\n",
    "        def denorm(x):\n",
    "            return (x * 0.5) + 0.5\n",
    "        \n",
    "        comparison = torch.cat([denorm(degraded), denorm(restored), denorm(clean)], dim=0)\n",
    "        \n",
    "        save_image(comparison, \n",
    "                   os.path.join(output_dir, f\"epoch_{epoch:03d}.png\"),\n",
    "                   nrow=num_samples, \n",
    "                   normalize=False) # Important de mettre False ici\n",
    "    \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aucun checkpoint trouv√©. D√©marrage d'un nouvel entra√Ænement.\n",
      "üîç Indexation des fichiers en cours...\n",
      "Indexation termin√©e : 66226 paires d'images trouv√©es.\n",
      "üîç Indexation des fichiers en cours...\n",
      "Indexation termin√©e : 16557 paires d'images trouv√©es.\n",
      "√âpoque 1/27 [Train]:   2%|‚ñè         | 33/2070 [00:55<56:39,  1.67s/it, L1=0.1234, VGG=0.6356] \n",
      "\n",
      "Arr√™t manuel (Ctrl+C). Sauvegarde de s√©curit√©...\n",
      "√âtat sauvegard√©. √Ä bient√¥t !\n",
      "\n",
      "S√©ance termin√©e !\n"
     ]
    }
   ],
   "source": [
    "# --- 5. MAIN TRAINING LOOP ---\n",
    "if __name__ == '__main__':\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialisation du mod√®le\n",
    "    model = VAE_UNet(latent_dim=64).to(device)\n",
    "    perceptual_fn = VGGPerceptualLoss().to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    # --- LOGIQUE DE REPRISE (RESUME) ---\n",
    "    checkpoint_path = \"vae_unet_last.pth\" if os.path.exists(\"vae_unet_last.pth\") else \"vae_unet_best.pth\"\n",
    "    start_epoch = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Chargement du checkpoint : {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        if 'optimizer_state_dict' in checkpoint:\n",
    "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            print(\"√âtat de l'optimiseur charg√©.\")\n",
    "            \n",
    "        start_epoch = checkpoint.get('epoch', 0)\n",
    "        best_val_loss = checkpoint.get('loss', float('inf'))\n",
    "        print(f\"Reprise du travail √† partir de l'√©poque {start_epoch + 1}\")\n",
    "    else:\n",
    "        print(\"Aucun checkpoint trouv√©. D√©marrage d'un nouvel entra√Ænement.\")\n",
    "\n",
    "    # --- LOGIQUE DE REPRISE DE L'HISTORIQUE (FIX VALUERROR) ---\n",
    "    history_path = \"training_history.csv\"\n",
    "    if os.path.exists(history_path):\n",
    "        try:\n",
    "            # On charge l'ancien CSV pour ne pas avoir de listes de tailles diff√©rentes\n",
    "            history = pd.read_csv(history_path).to_dict(orient='list')\n",
    "            print(f\"Historique charg√© ({len(history['epoch'])} √©poques trouv√©es).\")\n",
    "        except Exception:\n",
    "            history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'l1': [], 'vgg': [], 'kld': []}\n",
    "    else:\n",
    "        history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'l1': [], 'vgg': [], 'kld': []}\n",
    "\n",
    "    epochs_to_add = 27 \n",
    "    total_epochs = start_epoch + epochs_to_add\n",
    "\n",
    "    # --- DATALOADERS (Identiques) ---\n",
    "    # Define train_transform and val_transform\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    \n",
    "    dataset = RestorationDataset(\"../../data/train/degraded_images/\", \"../../data/train/images/\", train_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\n",
    "    val_dataset = RestorationDataset(\"../../data/test/degraded_images/\", \"../../data/test/images/\", val_transform)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    os.makedirs(\"samples\", exist_ok=True)\n",
    "\n",
    "    try:\n",
    "        for epoch in range(start_epoch, total_epochs):\n",
    "            model.train()\n",
    "            train_epoch_loss, epoch_l1, epoch_vgg, epoch_kld = 0, 0, 0, 0\n",
    "            train_bar = tqdm(dataloader, desc=f\"√âpoque {epoch+1}/{total_epochs} [Train]\", file=sys.stdout)\n",
    "            \n",
    "            for batch_idx, (degraded, clean) in enumerate(train_bar):\n",
    "                degraded, clean = degraded.to(device), clean.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                restored, mu, logvar = model(degraded)\n",
    "                loss, l1, vgg, kld = hybrid_loss(restored, clean, mu, logvar, epoch, perceptual_fn)\n",
    "                \n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_epoch_loss += loss.item()\n",
    "                epoch_l1 += l1; epoch_vgg += vgg; epoch_kld += kld\n",
    "                \n",
    "                if batch_idx % 10 == 0:\n",
    "                    train_bar.set_postfix({'L1': f'{l1:.4f}', 'VGG': f'{vgg:.4f}'})\n",
    "\n",
    "                if batch_idx % 500 == 0 and batch_idx > 0:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'loss': loss.item(),\n",
    "                    }, \"vae_unet_last.pth\")\n",
    "            \n",
    "            avg_train_loss = train_epoch_loss / len(dataloader)\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_epoch_loss = 0\n",
    "            val_bar = tqdm(val_dataloader, desc=f\"√âpoque {epoch+1}/{total_epochs} [Val]\", file=sys.stdout, leave=False)\n",
    "            with torch.no_grad():\n",
    "                for deg_v, cln_v in val_bar:\n",
    "                    deg_v, cln_v = deg_v.to(device), cln_v.to(device)\n",
    "                    res_v, mu_v, logvar_v = model(deg_v)\n",
    "                    v_loss, _, _, _ = hybrid_loss(res_v, cln_v, mu_v, logvar_v, epoch, perceptual_fn)\n",
    "                    val_epoch_loss += v_loss.item()\n",
    "            \n",
    "            avg_val_loss = val_epoch_loss / len(val_dataloader)\n",
    "            scheduler.step(avg_val_loss)\n",
    "\n",
    "            print(f\"\\nFIN √âPOQUE {epoch+1} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "            \n",
    "            # --- SAUVEGARDE √âTAT ---\n",
    "            save_dict = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': avg_val_loss,\n",
    "            }\n",
    "            torch.save(save_dict, \"vae_unet_last.pth\")\n",
    "\n",
    "            if avg_val_loss < best_val_loss:\n",
    "                best_val_loss = avg_val_loss\n",
    "                torch.save(save_dict, \"vae_unet_best.pth\")\n",
    "                print(f\"Nouveau record ! Mod√®le sauvegard√©.\")\n",
    "\n",
    "            # --- MISE √Ä JOUR HISTORIQUE (CORRECTION ICI) ---\n",
    "            history['epoch'].append(epoch + 1)\n",
    "            history['train_loss'].append(avg_train_loss)\n",
    "            history['val_loss'].append(avg_val_loss)\n",
    "            # ON AJOUTE LES M√âTRIQUES MANQUANTES POUR √âVITER LE VALUEERROR\n",
    "            history['l1'].append(epoch_l1 / len(dataloader))\n",
    "            history['vgg'].append(epoch_vgg / len(dataloader))\n",
    "            history['kld'].append(epoch_kld / len(dataloader))\n",
    "\n",
    "            # Sauvegarde propre du DataFrame\n",
    "            pd.DataFrame(history).to_csv(history_path, index=False)\n",
    "\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                save_samples(model, val_dataloader, epoch + 1, device)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nArr√™t manuel (Ctrl+C). Sauvegarde de s√©curit√©...\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_train_loss if 'avg_train_loss' in locals() else 0,\n",
    "        }, \"vae_unet_last.pth\")\n",
    "        print(\"√âtat sauvegard√©. √Ä bient√¥t !\")\n",
    "\n",
    "    print(\"\\nS√©ance termin√©e !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION TEST (CORRIG√âE) ---\n",
    "test_degraded_dir = \"../../data/test/degraded_images/\"\n",
    "test_clean_dir = \"../../data/test/images/\"\n",
    "output_dir = \"test_results_metrics\"\n",
    "checkpoint_path = \"vae_unet_best.pth\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. On utilise le MEME latent_dim que l'entra√Ænement (64)\n",
    "model = VAE_UNet(latent_dim=64).to(device) \n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint)\n",
    "model.eval()\n",
    "\n",
    "# 2. On utilise la MEME normalisation que l'entra√Ænement\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Fonction pour repasser en [0, 1] pour les calculs de PSNR/SSIM\n",
    "def denorm(x):\n",
    "    return (x * 0.5) + 0.5\n",
    "\n",
    "# --- BOUCLE DE TEST ---\n",
    "filenames = [f for f in os.listdir(test_degraded_dir) if f.endswith(('.jpg', '.png'))]\n",
    "all_psnr, all_ssim = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for filename in tqdm(filenames):\n",
    "        deg_path = os.path.join(test_degraded_dir, filename)\n",
    "        clean_filename = filename.replace(\"degraded_\", \"\")\n",
    "        clean_path = os.path.join(test_clean_dir, clean_filename)\n",
    "\n",
    "        if not os.path.exists(clean_path): continue\n",
    "\n",
    "        # On charge et on normalise\n",
    "        img_deg = test_transform(Image.open(deg_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "        img_clean = test_transform(Image.open(clean_path).convert('RGB')).unsqueeze(0).to(device)\n",
    "\n",
    "        # Inf√©rence\n",
    "        img_restored, _, _ = model(img_deg)\n",
    "\n",
    "        # 3. D√â-NORMALISATION pour comparer des images entre 0 et 1\n",
    "        clean_np = denorm(img_clean).squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "        restored_np = denorm(img_restored).squeeze().cpu().permute(1, 2, 0).numpy()\n",
    "\n",
    "        # Calcul des scores (sur plage 1.0)\n",
    "        current_psnr = psnr(clean_np, restored_np, data_range=1.0)\n",
    "        current_ssim = ssim(clean_np, restored_np, data_range=1.0, channel_axis=2)\n",
    "\n",
    "        all_psnr.append(current_psnr)\n",
    "        all_ssim.append(current_ssim)\n",
    "\n",
    "        # Sauvegarde visuelle d√©-normalis√©e\n",
    "        comparison = torch.cat([denorm(img_deg), denorm(img_restored), denorm(img_clean)], dim=0)\n",
    "        save_image(comparison, os.path.join(output_dir, f\"score_{current_psnr:.2f}_{filename}\"), nrow=3)\n",
    "\n",
    "print(f\"\\nPSNR Moyen : {np.mean(all_psnr):.2f} dB | SSIM Moyen : {np.mean(all_ssim):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
